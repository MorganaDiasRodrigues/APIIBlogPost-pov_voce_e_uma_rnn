Link do post no Blogger: [POV: Voc√™ √© uma Rede Recorrente](https://umblogparameuprof.blogspot.com/2024/04/redes-recorrentes.html)

Ok, ok, eu sei, eu tamb√©m queria estar aqui te ensinando sobre Transformers e LLMs e coisas m√°gicas que todo mundo no Linkedin t√° comentando. Mas, al√©m de que eu sou apenas uma iniciante na √°rea e n√£o possuo total conhecimento para te ofertar, jovem Padawan, eu quero te apresentar uma arquitetura mais antiga POR√âM, mais do que essencial pra gente ter chego t√£o longe nas arquiteturas.

Sim, √© ela mesma: a rede neural recorrente.

Neste blog post vamos conhecer essa diva ic√¥nica em termos te√≥ricos e depois vamos botar as m√£os na massa com o PyTorch!

#### O castigo do monstro: lidando com problemas sequenciais 
√â, o pessoal das MLPs e CNNs tamb√©m se fizeram esta mesma pergunta. Aqui, problemas sequenciais s√£o aqueles em que precisamos observar a ordem dos dados ao realizar a tarefa. O exemplo mais cl√°ssico de problemas que devem ser modelados de forma sequencial √© o processamento de linguagem natural, mas s√©ries temporais e sequ√™ncias gen√©ticas tamb√©m s√£o outros dois exemplos. Aqui, vamos focar em exemplos da √°rea de PLN para fins de praticidade (e porque eu gosto muito tamb√©m). üòâ

Imagina a seguinte situa√ß√£o:
Queremos fazer um levantamento de qualidade do conte√∫do do nosso blog a partir dos coment√°rios dos usu√°rios realizando uma classifica√ß√£o bin√°ria para dizer se o coment√°rio em quest√£o √© positivo ou negativo. Como sabemos, textos s√£o entradas sequenciais e este tipo de entrada j√° iria quebrar qualquer MLP pois estas s√≥ conseguem lidar com entradas de tamanho fixo e n√£o conseguiriam capturar o contexto dos coment√°rios, iria ser um desastre total!
"Ah, mas se o problema √© tamanho fixo, bora usar uma CNN da vida!"
√â, CNNs podem sim aceitar entradas com tamanhos variados, mas tem um pequeno grande detalhe: dependendo do tamanho da frase, ela pode acabar perdendo o contexto pois ter√≠amos que ficar adicionando muitas camadas convolucionais para aumentar o campo receptivo delas e eles serem capazes de capturar todo o contexto da frase. Al√©m do escopo do nosso problema, outras tarefas de PLN como tradu√ß√£o de texto exigem que as redes possam aceitar  cen√°rios onde temos uma entrada com um determinado tamanho e retornar uma sa√≠da com um tamanho diferente da entrada, e isso n√£o ocorre nessas arquiteturas.

Em terra de sequencinhas, quem guarda hidden states √© ra√≠nha
As RNRs surgiram justamente para resolver este problema que t√≠nhamos com as entradas (e sa√≠das) sequenciais. Isto porqu√™ essas queridas s√£o capazes de manter um estado que carrega as informa√ß√µes obtidas no passo anterior! Vamos conhecer um pouco melhor a arquitetura dela?

### Redes Neurais Recorrentes

![alt](img/RNN-legendada.png)

RNRs adicionam uma dimens√£o de tempo (representado aqui pelo subscrito t) e apresentam uma nova funcionalidade em sua arquitetura que n√≥s chamamos de estado interno ou, mais comumente, os hidden states. Eles s√£o respons√°veis por guardar o estado da rede no instante t ap√≥s processar a entrada X no mesmo instante. Pode parecer confuso lendo, ent√£o vamos dar uma abridinha nela usando como exemplo de entrada um coment√°rio fict√≠cio do blog.

![alt](img/comentario-blog.png)
![alt](img/RNN-desdobrada.png)

Assim √© uma RNR desdobrada no tempo. Dada a senten√ßa "As imagens est√£o boas", cada palavra da senten√ßa entra na rede no instante t, faz as computa√ß√µes internas e para o pr√≥ximo passo no instante t+1 ela envia o estado interno da √∫ltima entrada vista. Por isso elas s√£o ditas recorrentes: a sa√≠da de cada c√©lula da rede depende n√£o somente da entrada mas tamb√©m do estado interno da c√©lula anterior, criando este 'loop' que chamamos de recorr√™ncia.

---

### M√£os na massa com o PyTorch üë©‚Äçüíª
Aqui nesta se√ß√£o, vamos conhecer o PyTorch e o seu m√≥dulo dedicado a camadas RNN, o torch.nn.RNN

#### Quem √© o PyTorch na fila do p√£o?
O PyTorch se descreve oficialmente como uma biblioteca otimizada para se trabalhar com tensores em aplica√ß√µes de aprendizado profundo usando GPUs e CPUs, mas eu gosto de definir ele como um dos melhores (se n√£o o melhor) e mais popular framework para desenvolvimento de modelos de aprendizado profundo. Eles oferecem uma gama de funcionalidades dentro das Python APIs deles que no geral come√ßam com torch.[algum_m√≥dulo espec√≠fico]. Por exemplo, a que vamos utilizar aqui √© a torch.nn, uma API Python que oferece diversos building blocks para construir a rede que voc√™ deseja com as fun√ß√µes e otimizadores que voc√™ precisa.

#### torch.nn.RNN üòé
Esta camada computa, para cada elemento da sequ√™ncia de entrada, a seguinte fun√ß√£o:
![alt](img/tahn_hidden_state_torch.png)

Faremos a computa√ß√£o desta fun√ß√£o no nosso Jupyter Notebook na sess√£o Fala menos e faz mais para melhor entendimento, e l√° tamb√©m vou te apresentar o que significa cada elemento dela!
 
Primeiro precisamos entender o que tem nesta camada, o que ela recebe e o que ela retorna. 

##### Como funciona a camada 

**Par√¢metros**

Para instanciar um objeto torch.nn.RNN, devemos passar, pelo menos, os seguintes par√¢metros:

- input_size: representa o n√∫mero de features de entradas que vamos ter
- hidden_size: representa o n√∫mero de features da camada interna

Os outros par√¢metros j√° possuem um valor padr√£o caso voc√™ n√£o os coloque, que s√£o:

- num_layers: n√∫mero de camadas RNN que voc√™ deseja. Por default, 1 camada
nonlinearity: qual a fun√ß√£o n√£o-linear que queremos usar, podendo escolher entre a Tanh (que √© a default) e a 'ReLu'
- bias: um valor booleano para dizer se queres que a camada use ou n√£o os pesos de bias. Default √© que utilize, ou seja, True
- batch_first: valor booleano para indicar se queremos que os dados de entrada e de sa√≠da sej√£o providos no seguinte formato: (tamanho_do_batch, tamanho_da_sequ√™ncia, feature) ao inv√©s de (tamanho_da_sequ√™ncia, tamanho_do_batch, feature). O tamanho da sequ√™ncia geralmente se refere ao tamanho m√°ximo que a sequ√™ncia vai ter. Por exemplo, dado um vocabul√°rio sobre nomes, o maior nome tem 12 letras. 12 ser√° o tamanho_da_sequ√™ncia. Por default, ele vem como False.
- dropout: utilizado para dizer se queremos introduzir camas de Dropout nos outputs de cada camada RNN exceto a √∫ltima, sendo usada a probabilidade referenciada pelo pr√≥prio par√¢metro dropout. Quando ele vem 0.0, como √© o seu valor default, n√£o introduzimos. 
- bidirectional: valor booleano para indicar se a camada deve ser bidirecional o unidirecional. Por default, √© False e √© unidirecional.

**Entrada, sa√≠da e estado interno**

A entrada da camada deve ser um tensor com o seguinte formato, se batch_size=False:

- Tensor(tamanho_da_sequ√™ncia, batch_size, input_size)
A sa√≠da ser√° um tensor com o seguinte formato, se batch_size=False e bidirectional=False:
- Tensor(tamanho_da_sequ√™ncia, batch_size, hidden_size)
      Caso ela seja bidirecional, ser√°:
- Tensor(tamanho_da_sequ√™ncia, batch_size, 2 * hidden_size)
E o estado interno ser√° um tensor com o seguinte formato, se bidirectional=False
Tensor(n√∫mero_de_camadas, batch_size, hidden_size)

### Fala menos e faz mais (e uma pincelada de matem√°tica, pra ficar chique üíÖ) üíª
Para explorar melhor como funciona a rede, vamos instanciar uma RNN com uma √∫nica camada que vai receber um √∫nico dado de entrada (e consequentemente de sa√≠da) e vai ter um hidden state de apenas uma √∫nica feature tamb√©m. Ou seja, o t dela referente ao tempo, vai ir s√≥ at√© o 0, realizando somente uma √∫nica computa√ß√£o.
Instanciada a rede, vamos criar um dado de entrada e um hidden_state inicial, e passar para a camada realizar a computa√ß√£o da fun√ß√£o Tahn. 
Depois, vamos acessar todos os atributos  de peso e bias da camada para passar para esta mesma fun√ß√£o Tahn e conferir que o resultado √© o mesmo.

![alt](img/torch_rnn_example.png)

#### Gerando nomes com uma RNN ü§ñ

Agora que entendemos um pouco melhor com funcionam as camadas RNN do PyTorch, vamos ver um novo exemplo onde faremos a previs√£o do pr√≥ximo caractere para realizar a gera√ß√£o de nomes brasileiros!

##### Quer que desenhe? Pois eu tamb√©m!

Assim ir√° funcionar o treinamento da rede para gerar nomes:
Vamos usar como exemplo uma √∫nica inst√¢ncia de nome, o nome 'Lucas'.
Nosso vocabul√°rio ter√° 5 letras e ser√° decodificado pata n√∫meros e depois tensores.

![alt](img/rnn_vocab.png)
Al√©m disso, como as redes precisam de entradas num√©ricas, vamos fazer a representa√ß√£o desse nome como um vetor one-hot-encoding, desta forma

![alt](img/rnn_vocab_ohe.png)

Essa √© a representa√ß√£o que vai para a rede, ok?
Ent√£o, como queremos que ela fa√ßa previs√£o do pr√≥ximo caractere, e temos somente o nome 'Lucas', queremos que ela consiga predizer que ap√≥s o caractere 'L', a maior probabilidade para o pr√≥ximo caractere seja para o 'u', e assim por diante. Para isso, vamos dar como entrada o vetor OHE de cada letra e atrav√©s do algoritmo de backpropagation, vamos atualizar os pesos W os bias dela at√© que ela tenha um erro m√≠nimo entre o caractere que ela previu e o que de fato √© esperado.

![alt](img/rnn_training.png)

Depois da rede treinada, ela ser√° capaz de gerar o nome 'Lucas' pois estar√° com os pesos ajustados para isso!

![alt](img/rede%20treinada.png)

### Refer√™ncias:
Cap√≠tulo 48 ‚Äì Redes Neurais Recorrentes. (n.d.). In Deep Learning Book em Portugu√™s. Recuperado de https://www.deeplearningbook.com.br/redes-neurais-recorrentes/

Karpathy, A. (2015, 21 de maio). The Unreasonable Effectiveness of Recurrent Neural Networks Blog de Andrej Karpathy. Recuperado de https://karpathy.github.io/2015/05/21/rnn-effectiveness/

Loeber, P. (2020, 3 de setembro). PyTorch Tutorial - RNN & LSTM & GRU - Redes Neurais Recorrentes [V√≠deo]. YouTube. https://www.youtube.com/watch?v=0_PgWWmauHk&t=331s

Tamb√©m utilizei como refer√™ncia alguns materiais do meu professor Lucas Kupssinsku que leciona a cadeira de Aprendizado Profundo I e II na PUCRS.