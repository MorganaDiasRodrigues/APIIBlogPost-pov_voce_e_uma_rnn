!![alt](img/RNN-desdobrada.png)

**Link do post no Blogger: [POV: Voc√™ √© uma Rede Recorrente](https://umblogparameuprof.blogspot.com/2024/04/redes-recorrentes.html)**

Ok, ok, eu sei, eu tamb√©m queria estar aqui te ensinando sobre Transformers e LLMs e coisas m√°gicas que todo mundo no Linkedin t√° comentando. Mas, al√©m de que eu sou apenas uma iniciante na √°rea e n√£o possuo total conhecimento para te ofertar, jovem Padawan, eu quero te apresentar uma arquitetura mais antiga POR√âM, mais do que essencial pra gente ter chego t√£o longe nas arquiteturas.

Sim, √© ela mesma: a rede neural recorrente.

Neste blog post vamos conhecer essa diva ic√¥nica em termos te√≥ricos e depois vamos botar as m√£os na massa com o PyTorch!

#### O castigo do monstro: lidando com problemas sequenciais 
√â, o pessoal das MLPs e CNNs tamb√©m se fizeram esta mesma pergunta. Aqui, problemas sequenciais s√£o aqueles em que precisamos observar a ordem dos dados ao realizar a tarefa. O exemplo mais cl√°ssico de problemas que devem ser modelados de forma sequencial √© o processamento de linguagem natural, mas s√©ries temporais e sequ√™ncias gen√©ticas tamb√©m s√£o outros dois exemplos. Aqui, vamos focar em exemplos da √°rea de PLN para fins de praticidade (e porque eu gosto muito tamb√©m). üòâ

Imagina a seguinte situa√ß√£o:
Queremos fazer um levantamento de qualidade do conte√∫do do nosso blog a partir dos coment√°rios dos usu√°rios realizando uma classifica√ß√£o bin√°ria para dizer se o coment√°rio em quest√£o √© positivo ou negativo. Como sabemos, textos s√£o entradas sequenciais e este tipo de entrada j√° iria quebrar qualquer MLP pois estas s√≥ conseguem lidar com entradas de tamanho fixo e n√£o conseguiriam capturar o contexto dos coment√°rios, iria ser um desastre total!
"Ah, mas se o problema √© tamanho fixo, bora usar uma CNN da vida!"
√â, CNNs podem sim aceitar entradas com tamanhos variados, mas tem um pequeno grande detalhe: dependendo do tamanho da frase, ela pode acabar perdendo o contexto pois ter√≠amos que ficar adicionando muitas camadas convolucionais para aumentar o campo receptivo delas e eles serem capazes de capturar todo o contexto da frase. Al√©m do escopo do nosso problema, outras tarefas de PLN como tradu√ß√£o de texto exigem que as redes possam aceitar  cen√°rios onde temos uma entrada com um determinado tamanho e retornar uma sa√≠da com um tamanho diferente da entrada, e isso n√£o ocorre nessas arquiteturas.

Em terra de sequencinhas, quem guarda hidden states √© ra√≠nha
As RNRs surgiram justamente para resolver este problema que t√≠nhamos com as entradas (e sa√≠das) sequenciais. Isto porqu√™ essas queridas s√£o capazes de manter um estado que carrega as informa√ß√µes obtidas no passo anterior! Vamos conhecer um pouco melhor a arquitetura dela?

### Redes Neurais Recorrentes

![alt](img/RNN-legendada.png)

RNRs adicionam uma dimens√£o de tempo (representado aqui pelo subscrito t) e apresentam uma nova funcionalidade em sua arquitetura que n√≥s chamamos de estado interno ou, mais comumente, os hidden states. Eles s√£o respons√°veis por guardar o estado da rede no instante t ap√≥s processar a entrada X no mesmo instante. Pode parecer confuso lendo, ent√£o vamos dar uma abridinha nela usando como exemplo de entrada um coment√°rio fict√≠cio do blog.

![alt](img/comentario-blog.png)
![alt](img/RNN-desdobrada.png)

Assim √© uma RNR desdobrada no tempo. Dada a senten√ßa "As imagens est√£o boas", cada palavra da senten√ßa entra na rede no instante t, faz as computa√ß√µes internas e para o pr√≥ximo passo no instante t+1 ela envia o estado interno da √∫ltima entrada vista. Por isso elas s√£o ditas recorrentes: a sa√≠da de cada c√©lula da rede depende n√£o somente da entrada mas tamb√©m do estado interno da c√©lula anterior, criando este 'loop' que chamamos de recorr√™ncia.

---

### M√£os na massa com o PyTorch üë©‚Äçüíª
Aqui nesta se√ß√£o, vamos conhecer o PyTorch e o seu m√≥dulo dedicado a camadas RNN, o torch.nn.RNN

#### Quem √© o PyTorch na fila do p√£o?
O PyTorch se descreve oficialmente como uma biblioteca otimizada para se trabalhar com tensores em aplica√ß√µes de aprendizado profundo usando GPUs e CPUs, mas eu gosto de definir ele como um dos melhores (se n√£o o melhor) e mais popular framework para desenvolvimento de modelos de aprendizado profundo. Eles oferecem uma gama de funcionalidades dentro das Python APIs deles que no geral come√ßam com torch.[algum_m√≥dulo espec√≠fico]. Por exemplo, a que vamos utilizar aqui √© a torch.nn, uma API Python que oferece diversos building blocks para construir a rede que voc√™ deseja com as fun√ß√µes e otimizadores que voc√™ precisa.

#### torch.nn.RNN üòé
Esta camada computa, para cada elemento da sequ√™ncia de entrada, a seguinte fun√ß√£o:
![alt](img/tahn_hidden_state_torch.png)

Faremos a computa√ß√£o desta fun√ß√£o no nosso Jupyter Notebook na sess√£o Fala menos e faz mais para melhor entendimento, e l√° tamb√©m vou te apresentar o que significa cada elemento dela!
 
Primeiro precisamos entender o que tem nesta camada, o que ela recebe e o que ela retorna. 

##### Como funciona a camada 

**Par√¢metros**

Para instanciar um objeto torch.nn.RNN, devemos passar, pelo menos, os seguintes par√¢metros:

- input_size: representa o n√∫mero de features de entradas que vamos ter
- hidden_size: representa o n√∫mero de features da camada interna

Os outros par√¢metros j√° possuem um valor padr√£o caso voc√™ n√£o os coloque, que s√£o:

- num_layers: n√∫mero de camadas RNN que voc√™ deseja. Por default, 1 camada
nonlinearity: qual a fun√ß√£o n√£o-linear que queremos usar, podendo escolher entre a Tanh (que √© a default) e a 'ReLu'
- bias: um valor booleano para dizer se queres que a camada use ou n√£o os pesos de bias. Default √© que utilize, ou seja, True
- batch_first: valor booleano para indicar se queremos que os dados de entrada e de sa√≠da sej√£o providos no seguinte formato: (tamanho_do_batch, tamanho_da_sequ√™ncia, feature) ao inv√©s de (tamanho_da_sequ√™ncia, tamanho_do_batch, feature). O tamanho da sequ√™ncia geralmente se refere ao tamanho m√°ximo que a sequ√™ncia vai ter. Por exemplo, dado um vocabul√°rio sobre nomes, o maior nome tem 12 letras. 12 ser√° o tamanho_da_sequ√™ncia. Por default, ele vem como False.
- dropout: utilizado para dizer se queremos introduzir camas de Dropout nos outputs de cada camada RNN exceto a √∫ltima, sendo usada a probabilidade referenciada pelo pr√≥prio par√¢metro dropout. Quando ele vem 0.0, como √© o seu valor default, n√£o introduzimos. 
- bidirectional: valor booleano para indicar se a camada deve ser bidirecional o unidirecional. Por default, √© False e √© unidirecional.

**Entrada, sa√≠da e estado interno**

A entrada da camada deve ser um tensor com o seguinte formato, se batch_size=False:

- Tensor(tamanho_da_sequ√™ncia, batch_size, input_size)
A sa√≠da ser√° um tensor com o seguinte formato, se batch_size=False e bidirectional=False:
- Tensor(tamanho_da_sequ√™ncia, batch_size, hidden_size)
      Caso ela seja bidirecional, ser√°:
- Tensor(tamanho_da_sequ√™ncia, batch_size, 2 * hidden_size)
E o estado interno ser√° um tensor com o seguinte formato, se bidirectional=False
Tensor(n√∫mero_de_camadas, batch_size, hidden_size)

### Fala menos e faz mais (e uma pincelada de matem√°tica, pra ficar chique üíÖ) üíª
Para explorar melhor como funciona a rede, vamos instanciar uma RNN com uma √∫nica camada que vai receber um √∫nico dado de entrada (e consequentemente de sa√≠da) e vai ter um hidden state de apenas uma √∫nica feature tamb√©m. Ou seja, o t dela referente ao tempo, vai ir s√≥ at√© o 0, realizando somente uma √∫nica computa√ß√£o.
Instanciada a rede, vamos criar um dado de entrada e um hidden_state inicial, e passar para a camada realizar a computa√ß√£o da fun√ß√£o Tahn. 
Depois, vamos acessar todos os atributos  de peso e bias da camada para passar para esta mesma fun√ß√£o Tahn e conferir que o resultado √© o mesmo.

![alt](img/torch_rnn_example.png)

#### Camada RNN do torch.nn


```python
import torch
import torch.nn as nn
# Seed para reprodu√ß√£o de resultados
torch.manual_seed(42)

# Instanciando a rede
rnn = nn.RNN(1, 1, 1)
# tamanho_da_sequ√™ncia = 1, batch_size = 1, input_size = 1
input = torch.tensor([[[0.5]]])

# n√∫mero_de_camadas * 1, batch_size, hidden_size
hidden = torch.zeros(1, 1, 1)

# Passando a entrada e o estado oculto pela RNN
output, new_hidden = rnn(input, hidden)
# Resultado
print(f'Sa√≠da: {output[0][0][0]}')
print(f'Hidden state: {new_hidden[0][0][0]}')
```

    Sa√≠da: 0.788179874420166
    Hidden state: 0.788179874420166
    

#### Acessando os pesos e bias da rede para computar a fun√ß√£o Tahn

Como explicamos no blog, o que a camada realiza √© a computa√ß√£o da seguinte f√≥rmula:

$$
h_t = \tanh(x_t W_{ih}^T + b_{ih} + h_{t-1} W_{hh}^T + b_{hh})
$$


Onde os elementos de pesos e bias, representados por $W$ e $b$, colocados abaixo:


```python
# Acessando os pesos e bias da RNN
w_ih = rnn.weight_ih_l0  # Peso para a entrada para a camada oculta
w_hh = rnn.weight_hh_l0  # Peso para a camada oculta para a camada oculta
b_ih = rnn.bias_ih_l0    # Bias para a entrada para a camada oculta
b_hh = rnn.bias_hh_l0    # Bias para a camada oculta para a camada oculta
```

E o elemento $x_t$ da f√≥rmula se refere √† nossa entrada e o elemento $h_t$ se refere ao hidden_state

Como temos todos estes elementos, podemos calcular 'manualmente' a mesma fun√ß√£o com eles e observar o mesmo resultado de sa√≠da que a cadama RNN nos retornou.


```python
hidden_manual = torch.tanh(w_ih @ input[0] + b_ih + w_hh @ hidden + b_hh)
print(f'Resultado da f√≥rmula: {hidden_manual[0][0][0]}')
```

    Resultado da f√≥rmula: 0.788179874420166


#### Gerando nomes com uma RNN ü§ñ

Agora que entendemos um pouco melhor com funcionam as camadas RNN do PyTorch, vamos ver um novo exemplo onde faremos a previs√£o do pr√≥ximo caractere para realizar a gera√ß√£o de nomes brasileiros! 

# Gerando nomes brasileiros com RNNs

## Vamos fazer a previs√£o do pr√≥ximo caractere baseado em uma lista de nomes brasileiros! :)

#### Imports


```python
import torch
import torch.nn as nn
import torch.optim as optim
# seed, para reproducibilidade
torch.manual_seed(0)
```




    <torch._C.Generator at 0x18d425e53d0>



#### Carregando os dados


```python
def carrega_dados(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        nomes = file.read().strip().split('\n')
    return nomes
```


```python
nomes = carrega_dados('nomes_pt.txt')
print(f'5 primeiros nomes: {nomes[:5]}')
```

    5 primeiros nomes: ['Ana', 'Lucas', 'Maria', 'Jo√£o', 'Sofia']
    

#### Adicionando os tokens de In√≠cio e Fim dos nomes

Para ajudar o modelo a entender onde come√ßou e onde terminou o nome.


```python
nomes = ['b/' + nome + '/e' for nome in nomes]
print(nomes[:5])
```

    ['b/Ana/e', 'b/Lucas/e', 'b/Maria/e', 'b/Jo√£o/e', 'b/Sofia/e']
    

#### Criando o vocabul√°rio, o encoder e o decoder


```python
vocab = sorted(set(''.join(nomes))) # Aqui, o vocabul√°rio √© o conjunto de todos os caracteres √∫nicos presentes nos nomes
print(f'Tamanho do vocabul√°rio: {len(vocab)} caracteres √∫nicos')
encoder = {ch: i for i, ch in enumerate(vocab)} # Aqui, criamos um dicion√°rio que mapeia cada caractere para um √≠ndice
decoder = {i: ch for i, ch in enumerate(vocab)} # Aqui, criamos um dicion√°rio que mapeia cada √≠ndice para um caractere
```

    Tamanho do vocabul√°rio: 47 caracteres √∫nicos
    


```python
print(f'A letra L est√° codificada como o n√∫mero {encoder["L"]}')
```

    A letra L est√° codificada como o n√∫mero 11
    

### Fun√ß√µes auxiliares para o pr√©-processamento dos dados

#### Fun√ß√£o para mapear cada caractere codificado para um tensor


```python
def char_tensor(string):
    tensor = torch.zeros(len(string)).long()
    for c in range(len(string)):
        tensor[c] = encoder[string[c]]
    return tensor
```


```python
print(f'Nome Lucas codificado e em tensor: {char_tensor("Lucas")}')
```

    Nome Lucas codificado e em tensor: tensor([11, 37, 21, 19, 35])
    

#### Aplicando OHE


```python
def one_hot_encoding(tensor):
    return torch.nn.functional.one_hot(tensor, num_classes=len(vocab)).type(torch.float32) # Vai retornar um tensor com o tamanho do vocabul√°rio, com 1s nas posi√ß√µes correspondentes aos caracteres presentes no tensor de entrada

```


```python
ohe_exemplo = one_hot_encoding(char_tensor('Lucas'))
print(f'One-hot encoding do nome Lucas: {ohe_exemplo} \n')
# Veja que em algumas posi√ß√µes, h√° o n√∫mero 1. Signifa que ali havia alguma letra correspondente ao nome 'Lucas'
print(f'Tamanho do one-hot encoding: {ohe_exemplo.size()}')
```

    One-hot encoding do nome Lucas: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) 
    
    Tamanho do one-hot encoding: torch.Size([5, 47])
    

# Modelo RNN para previs√£o do pr√≥ximo caractere


```python
class CharRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CharRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # Olha aqui nossa diva! O shape dela ser√° (tamanho_do_vocab, 100, tamanho_do_vocab)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, input, hidden):
        out, hidden = self.rnn(input, hidden)
        out = self.fc(out[:, -1, :]) # Pega o √∫ltimo output da sequ√™ncia
        return out, hidden # Sempre retornar hidden para a pr√≥xima itera√ß√£o

    def init_hidden(self, batch_size):
        return torch.zeros(1, batch_size, hidden_size) # Vamos inicializar o hidden com zeros.
```

#### Instanciando o modelo


```python
hidden_size = 100
model = CharRNN(len(vocab), hidden_size, len(vocab))
```

#### Definindo nossa fun√ß√£o de perda e nosso otimizador


```python
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)
```

#### Fun√ß√£o de treino


```python
def train(input_line_tensor, target_char_tensor, device):
    if device == 'cuda':
        input_line_tensor = input_line_tensor.cuda()
        target_char_tensor = target_char_tensor.cuda()
        model.to(device)
    hidden = model.init_hidden(1)
    hidden = hidden.to(device)
    model.zero_grad()
    loss = 0

    for i in range(input_line_tensor.size(0) - 1):
        input = input_line_tensor[i:i+1].unsqueeze(0)
        target = target_char_tensor[i+1]
        output, hidden = model(input, hidden)
        loss += loss_fn(output, target.view(1))

    loss.backward()
    optimizer.step()

    return loss.item() / (input_line_tensor.size(0) - 1)
```

### Loop de treinamento


```python
EPOCHS = 300 # N√∫mero de √©pocas que vamos treinar

if torch.cuda.is_available():
    device = 'cuda' # Usaremos a GPU se dispon√≠vel

for epoch in range(EPOCHS):
    for name in nomes:
        name_tensor = one_hot_encoding(char_tensor(name))
        name_tensor.to(device)
        loss = train(name_tensor, char_tensor(name), device=device)
    if epoch % 50 == 0:
        print(f'Epoch: {epoch}, Loss: {loss:.4f}')
```

    Epoch: 0, Loss: 2.2929
    Epoch: 50, Loss: 0.6604
    Epoch: 100, Loss: 0.6915
    Epoch: 150, Loss: 1.0655
    Epoch: 200, Loss: 0.6139
    Epoch: 250, Loss: 0.6143
    

### A melhor hora: Vamos fazer infer√™ncias com nosso modelo!

#### Fun√ß√£o para gera√ß√£o de nomes


```python
def generate(initial_char, max_length=12): # Initial_char √© a letra que a rede vai usar para come√ßar a gerar nomes e o max_length √© o tamanho m√°ximo do nome gerado. Eu definir 12, mas voc√™ pode mudar.
    with torch.no_grad(): 
        hidden = model.init_hidden(1).to(device)
        input_char_tensor = one_hot_encoding(char_tensor(initial_char)).to(device)
        predicted_name = initial_char

        for _ in range(max_length - 1):
            input = input_char_tensor.unsqueeze(0)
            output, hidden = model(input, hidden)

            _ , topi = output.topk(1) # Vamos pegar o √≠ndice do maior valor, que √© a previs√£o da rede para o pr√≥ximo caractere
            char_index = topi[0][0].item()
            if char_index == encoder[initial_char]:
                break
            else:
                char = decoder[char_index]
                predicted_name += char
                input_char_tensor = one_hot_encoding(char_tensor(char)).to(device)

        return predicted_name
```

### Gerando nomes üòÅ


```python
model = model.to(device) # Colocando o modelo na GPU
model.eval()  # E colocando ele em modo de avalia√ß√£o
```




    CharRNN(
      (rnn): RNN(47, 100, batch_first=True)
      (fc): Linear(in_features=100, out_features=47, bias=True)
    )




```python
start_char = 'L'
predicted_name = generate(start_char)
# Pegar os caractres at√© o /
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'L': L√≠via
    


```python
start_char = 'M'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'M': Mardo
    


```python
start_char = 'D'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'D': Danciss
    


```python
start_char = 'P'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'P': Pedo√≠sna
    


```python
start_char = 'Y'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'Y': Yasmindo
    

### Conclus√£o:

Por esta lista ser super curta, somente com 70 nomes, o modelo tem algumas limita√ß√µes como n√£o conseguir gerar nomes que come√ßam com O (porque n√£o tem nomes com essa letra na lista) al√©m de ter pocuso exemplares de nomes diversos.

Por√©m o intuito aqui era apenas exemplificar o uso da RNN :)


##### Quer que desenhe? Pois eu tamb√©m!

Assim ir√° funcionar o treinamento da rede para gerar nomes:
Vamos usar como exemplo uma √∫nica inst√¢ncia de nome, o nome 'Lucas'.
Nosso vocabul√°rio ter√° 5 letras e ser√° decodificado pata n√∫meros e depois tensores.

![alt](img/rnn_vocab.png)
Al√©m disso, como as redes precisam de entradas num√©ricas, vamos fazer a representa√ß√£o desse nome como um vetor one-hot-encoding, desta forma

![alt](img/rnn_vocab_ohe.png)

Essa √© a representa√ß√£o que vai para a rede, ok?
Ent√£o, como queremos que ela fa√ßa previs√£o do pr√≥ximo caractere, e temos somente o nome 'Lucas', queremos que ela consiga predizer que ap√≥s o caractere 'L', a maior probabilidade para o pr√≥ximo caractere seja para o 'u', e assim por diante. Para isso, vamos dar como entrada o vetor OHE de cada letra e atrav√©s do algoritmo de backpropagation, vamos atualizar os pesos W os bias dela at√© que ela tenha um erro m√≠nimo entre o caractere que ela previu e o que de fato √© esperado.

![alt](img/rnn_training.png)

Depois da rede treinada, ela ser√° capaz de gerar o nome 'Lucas' pois estar√° com os pesos ajustados para isso!

![alt](img/rede%20treinada.png)

### Refer√™ncias:
Cap√≠tulo 48 ‚Äì Redes Neurais Recorrentes. (n.d.). In Deep Learning Book em Portugu√™s. Recuperado de https://www.deeplearningbook.com.br/redes-neurais-recorrentes/

Karpathy, A. (2015, 21 de maio). The Unreasonable Effectiveness of Recurrent Neural Networks Blog de Andrej Karpathy. Recuperado de https://karpathy.github.io/2015/05/21/rnn-effectiveness/

Loeber, P. (2020, 3 de setembro). PyTorch Tutorial - RNN & LSTM & GRU - Redes Neurais Recorrentes [V√≠deo]. YouTube. https://www.youtube.com/watch?v=0_PgWWmauHk&t=331s

Tamb√©m utilizei como refer√™ncia alguns materiais do meu professor Lucas Kupssinsku que leciona a cadeira de Aprendizado Profundo I e II na PUCRS.