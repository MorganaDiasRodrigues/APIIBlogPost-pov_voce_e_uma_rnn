# Gerando nomes brasileiros com RNNs

## Vamos fazer a previs√£o do pr√≥ximo caractere baseado em uma lista de nomes brasileiros! :)

#### Imports


```python
import torch
import torch.nn as nn
import torch.optim as optim
# seed, para reproducibilidade
torch.manual_seed(0)
```




    <torch._C.Generator at 0x18d425e53d0>



#### Carregando os dados


```python
def carrega_dados(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        nomes = file.read().strip().split('\n')
    return nomes
```


```python
nomes = carrega_dados('nomes_pt.txt')
print(f'5 primeiros nomes: {nomes[:5]}')
```

    5 primeiros nomes: ['Ana', 'Lucas', 'Maria', 'Jo√£o', 'Sofia']
    

#### Adicionando os tokens de In√≠cio e Fim dos nomes

Para ajudar o modelo a entender onde come√ßou e onde terminou o nome.


```python
nomes = ['b/' + nome + '/e' for nome in nomes]
print(nomes[:5])
```

    ['b/Ana/e', 'b/Lucas/e', 'b/Maria/e', 'b/Jo√£o/e', 'b/Sofia/e']
    

#### Criando o vocabul√°rio, o encoder e o decoder


```python
vocab = sorted(set(''.join(nomes))) # Aqui, o vocabul√°rio √© o conjunto de todos os caracteres √∫nicos presentes nos nomes
print(f'Tamanho do vocabul√°rio: {len(vocab)} caracteres √∫nicos')
encoder = {ch: i for i, ch in enumerate(vocab)} # Aqui, criamos um dicion√°rio que mapeia cada caractere para um √≠ndice
decoder = {i: ch for i, ch in enumerate(vocab)} # Aqui, criamos um dicion√°rio que mapeia cada √≠ndice para um caractere
```

    Tamanho do vocabul√°rio: 47 caracteres √∫nicos
    


```python
print(f'A letra L est√° codificada como o n√∫mero {encoder["L"]}')
```

    A letra L est√° codificada como o n√∫mero 11
    

### Fun√ß√µes auxiliares para o pr√©-processamento dos dados

#### Fun√ß√£o para mapear cada caractere codificado para um tensor


```python
def char_tensor(string):
    tensor = torch.zeros(len(string)).long()
    for c in range(len(string)):
        tensor[c] = encoder[string[c]]
    return tensor
```


```python
print(f'Nome Lucas codificado e em tensor: {char_tensor("Lucas")}')
```

    Nome Lucas codificado e em tensor: tensor([11, 37, 21, 19, 35])
    

#### Aplicando OHE


```python
def one_hot_encoding(tensor):
    return torch.nn.functional.one_hot(tensor, num_classes=len(vocab)).type(torch.float32) # Vai retornar um tensor com o tamanho do vocabul√°rio, com 1s nas posi√ß√µes correspondentes aos caracteres presentes no tensor de entrada

```


```python
ohe_exemplo = one_hot_encoding(char_tensor('Lucas'))
print(f'One-hot encoding do nome Lucas: {ohe_exemplo} \n')
# Veja que em algumas posi√ß√µes, h√° o n√∫mero 1. Signifa que ali havia alguma letra correspondente ao nome 'Lucas'
print(f'Tamanho do one-hot encoding: {ohe_exemplo.size()}')
```

    One-hot encoding do nome Lucas: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) 
    
    Tamanho do one-hot encoding: torch.Size([5, 47])
    

# Modelo RNN para previs√£o do pr√≥ximo caractere


```python
class CharRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CharRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # Olha aqui nossa diva! O shape dela ser√° (tamanho_do_vocab, 100, tamanho_do_vocab)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, input, hidden):
        out, hidden = self.rnn(input, hidden)
        out = self.fc(out[:, -1, :]) # Pega o √∫ltimo output da sequ√™ncia
        return out, hidden # Sempre retornar hidden para a pr√≥xima itera√ß√£o

    def init_hidden(self, batch_size):
        return torch.zeros(1, batch_size, hidden_size) # Vamos inicializar o hidden com zeros.
```

#### Instanciando o modelo


```python
hidden_size = 100
model = CharRNN(len(vocab), hidden_size, len(vocab))
```

#### Definindo nossa fun√ß√£o de perda e nosso otimizador


```python
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)
```

#### Fun√ß√£o de treino


```python
def train(input_line_tensor, target_char_tensor, device):
    if device == 'cuda':
        input_line_tensor = input_line_tensor.cuda()
        target_char_tensor = target_char_tensor.cuda()
        model.to(device)
    hidden = model.init_hidden(1)
    hidden = hidden.to(device)
    model.zero_grad()
    loss = 0

    for i in range(input_line_tensor.size(0) - 1):
        input = input_line_tensor[i:i+1].unsqueeze(0)
        target = target_char_tensor[i+1]
        output, hidden = model(input, hidden)
        loss += loss_fn(output, target.view(1))

    loss.backward()
    optimizer.step()

    return loss.item() / (input_line_tensor.size(0) - 1)
```

### Loop de treinamento


```python
EPOCHS = 300 # N√∫mero de √©pocas que vamos treinar

if torch.cuda.is_available():
    device = 'cuda' # Usaremos a GPU se dispon√≠vel

for epoch in range(EPOCHS):
    for name in nomes:
        name_tensor = one_hot_encoding(char_tensor(name))
        name_tensor.to(device)
        loss = train(name_tensor, char_tensor(name), device=device)
    if epoch % 50 == 0:
        print(f'Epoch: {epoch}, Loss: {loss:.4f}')
```

    Epoch: 0, Loss: 2.2929
    Epoch: 50, Loss: 0.6604
    Epoch: 100, Loss: 0.6915
    Epoch: 150, Loss: 1.0655
    Epoch: 200, Loss: 0.6139
    Epoch: 250, Loss: 0.6143
    

### A melhor hora: Vamos fazer infer√™ncias com nosso modelo!

#### Fun√ß√£o para gera√ß√£o de nomes


```python
def generate(initial_char, max_length=12): # Initial_char √© a letra que a rede vai usar para come√ßar a gerar nomes e o max_length √© o tamanho m√°ximo do nome gerado. Eu definir 12, mas voc√™ pode mudar.
    with torch.no_grad(): 
        hidden = model.init_hidden(1).to(device)
        input_char_tensor = one_hot_encoding(char_tensor(initial_char)).to(device)
        predicted_name = initial_char

        for _ in range(max_length - 1):
            input = input_char_tensor.unsqueeze(0)
            output, hidden = model(input, hidden)

            _ , topi = output.topk(1) # Vamos pegar o √≠ndice do maior valor, que √© a previs√£o da rede para o pr√≥ximo caractere
            char_index = topi[0][0].item()
            if char_index == encoder[initial_char]:
                break
            else:
                char = decoder[char_index]
                predicted_name += char
                input_char_tensor = one_hot_encoding(char_tensor(char)).to(device)

        return predicted_name
```

### Gerando nomes üòÅ


```python
model = model.to(device) # Colocando o modelo na GPU
model.eval()  # E colocando ele em modo de avalia√ß√£o
```




    CharRNN(
      (rnn): RNN(47, 100, batch_first=True)
      (fc): Linear(in_features=100, out_features=47, bias=True)
    )




```python
start_char = 'L'
predicted_name = generate(start_char)
# Pegar os caractres at√© o /
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'L': L√≠via
    


```python
start_char = 'M'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'M': Mardo
    


```python
start_char = 'D'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'D': Danciss
    


```python
start_char = 'P'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'P': Pedo√≠sna
    


```python
start_char = 'Y'
predicted_name = generate(start_char)
print(f"Nome gerado com o caractere '{start_char}': {predicted_name.split('/')[0]}")
```

    Nome gerado com o caractere 'Y': Yasmindo
    

### Conclus√£o:

Por esta lista ser super curta, somente com 70 nomes, o modelo tem algumas limita√ß√µes como n√£o conseguir gerar nomes que come√ßam com O (porque n√£o tem nomes com essa letra na lista) al√©m de ter pocuso exemplares de nomes diversos.

Por√©m o intuito aqui era apenas exemplificar o uso da RNN :)
